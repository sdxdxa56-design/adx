"""
scripts/index_faiss.py - ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙÙŠ FAISS
"""

import os
import sys
import json
import argparse
from pathlib import Path

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ø£Ø¯ÙˆØ§Øª
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sentence_transformers import SentenceTransformer
import faiss
from config import Config

def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> list:
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù…ØªØ¯Ø§Ø®Ù„Ø©"""
    words = text.split()
    chunks = []
    
    i = 0
    while i < len(words):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
        i += chunk_size - overlap
    
    return chunks

def index_documents(data_dir: str = "data", 
                   index_file: str = "faiss.index",
                   meta_file: str = "metadata.json",
                   model_name: str = "all-MiniLM-L6-v2"):
    """ÙÙ‡Ø±Ø³Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù†ØµÙŠØ©"""
    
    print("ðŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙÙ‡Ø±Ø³Ø©...")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = SentenceTransformer(model_name)
    print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_name}")
    
    texts = []
    metadata = []
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ©
    data_path = Path(data_dir)
    txt_files = list(data_path.glob("**/*.txt")) + list(data_path.glob("**/*.json"))
    
    print(f"ðŸ” ÙˆØ¬Ø¯Øª {len(txt_files)} Ù…Ù„Ù Ù„Ù„ÙÙ‡Ø±Ø³Ø©")
    
    for file_path in txt_files:
        try:
            print(f"ðŸ“– Ù…Ø¹Ø§Ù„Ø¬Ø©: {file_path.name}")
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
            country = "unknown"
            doc_type = "unknown"
            
            if "yemen" in file_path.name.lower():
                country = "Yemen"
            elif "egypt" in file_path.name.lower():
                country = "Egypt"
            elif "saudi" in file_path.name.lower():
                country = "Saudi Arabia"
            
            if "civil" in file_path.name.lower():
                doc_type = "Ù‚Ø§Ù†ÙˆÙ† Ù…Ø¯Ù†ÙŠ"
            elif "criminal" in file_path.name.lower():
                doc_type = "Ù‚Ø§Ù†ÙˆÙ† Ø¬Ù†Ø§Ø¦ÙŠ"
            elif "labor" in file_path.name.lower():
                doc_type = "Ù‚Ø§Ù†ÙˆÙ† Ø¹Ù…Ù„"
            elif "commercial" in file_path.name.lower():
                doc_type = "Ù‚Ø§Ù†ÙˆÙ† ØªØ¬Ø§Ø±ÙŠ"
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ
            chunks = chunk_text(content)
            
            for i, chunk in enumerate(chunks):
                texts.append(chunk)
                metadata.append({
                    "source": str(file_path),
                    "country": country,
                    "type": doc_type,
                    "chunk": i,
                    "preview": chunk[:100],
                    "title": f"{doc_type} - {country}"
                })
                
            print(f"  âœ“ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© {len(chunks)} Ø¬Ø²Ø¡")
            
        except Exception as e:
            print(f"  âœ— Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {file_path.name}: {e}")
    
    if not texts:
        print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØµÙˆØµ Ù„Ù„ÙÙ‡Ø±Ø³Ø©!")
        return False
    
    print(f"ðŸ”¢ Ø¬Ø§Ø±ÙŠ ØªØ¶Ù…ÙŠÙ† {len(texts)} Ø¬Ø²Ø¡ Ù†ØµÙŠ...")
    
    # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ
    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    
    # Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³
    faiss.write_index(index, index_file)
    
    # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
    with open(meta_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ù†Ø¬Ø§Ø­!")
    print(f"ðŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
    print(f"   - Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡: {len(texts)}")
    print(f"   - Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {dim}")
    print(f"   - Ø­Ø¬Ù… Ø§Ù„ÙÙ‡Ø±Ø³: {os.path.getsize(index_file) / (1024*1024):.2f} MB")
    print(f"   - Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³: {index_file}")
    print(f"   - Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {meta_file}")
    
    return True

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙÙŠ FAISS")
    parser.add_argument("--data-dir", default="data", help="Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    parser.add_argument("--index-file", default=Config.FAISS_INDEX_PATH, help="Ù…Ø³Ø§Ø± Ø§Ù„ÙÙ‡Ø±Ø³")
    parser.add_argument("--meta-file", default=Config.FAISS_METADATA_PATH, help="Ù…Ø³Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©")
    parser.add_argument("--model", default="all-MiniLM-L6-v2", help="Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†")
    
    args = parser.parse_args()
    
    success = index_documents(
        data_dir=args.data_dir,
        index_file=args.index_file,
        meta_file=args.meta_file,
        model_name=args.model
    )
    
    if success:
        print("ðŸŽ‰ ØªÙ…Øª Ø§Ù„ÙÙ‡Ø±Ø³Ø© Ø¨Ù†Ø¬Ø§Ø­!")
        sys.exit(0)
    else:
        print("âŒ ÙØ´Ù„Øª Ø§Ù„ÙÙ‡Ø±Ø³Ø©!")
        sys.exit(1)